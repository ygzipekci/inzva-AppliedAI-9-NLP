{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f633f33b-3d3f-4645-8f69-b9ef7638060f",
   "metadata": {},
   "source": [
    "## Dinozor NLP "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5df7d2c1-f23f-42b5-bf1a-ed72024d1327",
   "metadata": {},
   "source": [
    "Our goal is to to demonstrate an old NLP task with old NLP methodologies, to understand what the future methods are trying to do better. For this goal I found a Turkish SMS Spam detection dataset from [Onur Karasoy et al.](https://github.com/onrkrsy/TurkishSMS-Collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f97e03-dd8e-4edb-9d37-247221cde9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "dataset_path = Path(\"TurkishSMS-Collection/TurkishSMSCollection.csv\")\n",
    "df = pd.read_csv(dataset_path, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a38e12-7144-4496-9ce1-562a91140ae2",
   "metadata": {},
   "source": [
    "This is how the data looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea82aab-b032-420f-ba5c-32f561ee95fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4340e55-bcc3-4a5d-85f5-28851940a244",
   "metadata": {},
   "source": [
    "The classes are balanced which is nice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b092cde8-c75e-4f1e-85b8-d62b5e381ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Group.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36726fd-758b-4a13-be3e-73687b97c157",
   "metadata": {},
   "source": [
    "I am turning classes into 0 and 1 for convenience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711a3e12-f1fe-4272-b456-24170a70b170",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Group\"] = df[\"Group\"].replace(2, 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "40852678-02b5-49f0-9c33-5f43a5092d50",
   "metadata": {},
   "source": [
    "### Good Old Feature Engineering\n",
    "\n",
    "Please Look at some samples and try to come up with some features that distinguish between spam and ham in this dataset. Than a bunch of classifiers will take those as inputs to generate scores. It is not about the results but the process of applying ancient ML methodologies for real life NLP problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89224001-3a10-403f-8eeb-d8e19ab996b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c14faf-3865-42ad-a2a0-a9c7fb825811",
   "metadata": {},
   "source": [
    "Take a look at random samples from each classes and try to come up with features that differanciates onw from the other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2f08f2-cc11-484a-b685-752a8d9815d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"Group\"] == 1].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158ba71b-fa5c-45ca-a297-a2473f5243f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"Group\"] == 0].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b98e6c2-3dad-4f63-9ee5-e2009911c3b9",
   "metadata": {},
   "source": [
    "Below here, engineer some features and append them to the original dataframe like:\n",
    "\n",
    "```python\n",
    "def get_my_feature(text):\n",
    "    # calculate your galaxy brain feature\n",
    "    text = text.do_stuff()\n",
    "    return text\n",
    "\n",
    "df[\"my_feature\"] = df[\"Messages\"].apply(get_my_feature)\n",
    "```\n",
    "or in any way you like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967f1da1-7494-4175-a020-6a4f5203bf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Remove special characters and standardize the text.\"\"\"\n",
    "    # Remove special characters except alphanumeric and spaces\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\sİıÜüÖöŞşÇçĞğ]', '', text)\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Make everthing lowercase\n",
    "    text = text.replace(\"I\", \"ı\").replace(\"İ\", \"i\").lower()\n",
    "    # Strip leading/trailing whitespace\n",
    "    return text.strip()\n",
    "\n",
    "def get_caps_proportion(text: str) -> float:\n",
    "    \"\"\"Return the percentage of uppercase characters in the given string.\"\"\"\n",
    "    caps_count = sum(1 for char in text if char.isupper())\n",
    "    return caps_count / len(text)\n",
    "\n",
    "def count_urls(text: str) -> int:\n",
    "    \"\"\"Return the number of URLs found in the given string.\"\"\"\n",
    "    url_pattern = r'https?://\\S+'\n",
    "    return len(re.findall(url_pattern, text))\n",
    "\n",
    "def count_numbers(text: str) -> int:\n",
    "    \"\"\"Return the number of numbers found in the given string.\"\"\"\n",
    "    number_pattern = r'\\d+'\n",
    "    return len(re.findall(number_pattern, text))\n",
    "\n",
    "def numeric_char_proportion(text: str) -> float:\n",
    "    \"\"\"Return the percentage of numeric characters in the given string.\"\"\"\n",
    "    return sum(1 for char in text if char.isdigit()) / len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78ebd33-c9ce-40ec-af79-e8f6b9c10dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c22e3f-530c-4394-9f8d-08c35facb3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"uppercase_proportion\"] = df[\"Message\"].progress_apply(get_caps_proportion)\n",
    "df[\"url_count\"] = df[\"Message\"].progress_apply(count_urls)\n",
    "df[\"number_count\"] = df[\"Message\"].progress_apply(count_numbers)\n",
    "df[\"numeric_char_proportion\"] = df[\"Message\"].progress_apply(numeric_char_proportion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc18ef1f-2a10-4f1f-9fce-475e426c3163",
   "metadata": {},
   "source": [
    "take a look at your newly engineered features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c069d61a-3233-41a4-be04-5479eeffa3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8315cdc3-f406-4b15-8532-6e6abf325757",
   "metadata": {},
   "source": [
    "### Lazy classifier\n",
    "\n",
    "Grinding different models to hit a higher score is automatable. What we are doing here is only meaningful during benchmarking, productionizing our solutions would bring up different concerns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc702a57-4720-4ff8-ac2f-4fa0ce7b4938",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(df: pd.DataFrame, target: str, ratio: float=0.3): # i know i didn't need to write this\n",
    "    X = df.drop(target, axis=1)\n",
    "    Y = df[[target]]\n",
    "    split = round(len(df)*ratio)\n",
    "    X_test = X.iloc[:split]\n",
    "    X_train = X.iloc[split:]\n",
    "    y_test = Y.iloc[:split]\n",
    "    y_train = Y.iloc[split:]\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8bd770-02c2-46af-96a8-9835dd54c0a0",
   "metadata": {},
   "source": [
    "I am using a library called lazy predict which is basically goes around and tries every sklearn classifier on your data, so that we can ignore model selection and hyperparameter tuning and just focus on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bcc451-6d5a-4dd3-80e9-b0e3c06c97e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lazypredict.Supervised import LazyClassifier\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "## write here your features along with the Group column\n",
    "features = df[[\"uppercase_proportion\", \"url_count\", \"number_count\", \"numeric_char_proportion\", \"Group\"]]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target=\"Group\", ratio=.3)\n",
    "\n",
    "clf = LazyClassifier(verbose=0, ignore_warnings=False, predictions=True, random_state=42, classifiers=\"all\", custom_metric=precision_score)\n",
    "models, predictions = clf.fit(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ae99a2-c79e-4565-8307-6a3294f76106",
   "metadata": {},
   "source": [
    "Let's make up a business rule and say that our spam filtering system must avoid flagging our loved ones' SMSs as spams."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece9a345-39b2-4a06-92b2-808984f8e208",
   "metadata": {},
   "source": [
    "So let's compare classifiers by precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818a389d-3da7-446c-adb0-3e100e473a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "models.sort_values(\"precision_score\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0859145-70fa-4596-9859-ba66a61ae455",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "418752ad-fdf6-4769-ac95-667d56f10d74",
   "metadata": {},
   "source": [
    "Here is a simple error analysis view so you can see what sort of examples are predicted wrong and develop features based on your hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed341d0-f865-4fcd-b27b-0cdf318d0227",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[\"Group\"] = y_test\n",
    "highest_precision_classifier = models.sort_values(\"precision_score\", ascending=False).index[0]\n",
    "\n",
    "# the examples that were not spams but the best classifier decided otherwise\n",
    "indices = predictions[(predictions[\"Group\"] == 0) & (predictions[highest_precision_classifier] == 1)].index\n",
    "df.iloc[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5226b878-8058-44b3-bc7d-baa1d41f7ff6",
   "metadata": {},
   "source": [
    "What do you think could be improved?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b11a1552-7a02-436e-951f-125aa6f8e6ae",
   "metadata": {},
   "source": [
    "### Term document matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a9cf3dcc-81b4-4b9e-95d1-2ff15360a582",
   "metadata": {},
   "source": [
    "Since the dawn of time, the goal of NLP research is to somehow represent language units with numbers. Because only then, we can make data science with them"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f9af1fb0-a6eb-40d8-8159-1d177b66b07b",
   "metadata": {},
   "source": [
    "![xkcd](https://imgs.xkcd.com/comics/assigning_numbers.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dfdf5396-c8e4-4968-91cd-14e64f074689",
   "metadata": {},
   "source": [
    "One of the older ways to represent words (or tokens) and documents was to create a term-document matrix. We can assume in such matrix the rows are words and columns are documents, and the cells are a function of those two. The most basic function to use might be the frequency of that word in a document. With that we are representing each document with a vacabulary-size dimentional sparse vector. It is also called a co-occurance matrix. Words that co-occur are represented by vectors that are closer.  \n",
    "For example the words \"volkan\" and \"konak\" might occur together more often than \"volkan\" and \"şemsiye\"; Therefore distance(\"volkan\", \"konak\") < distance(\"volkan\", \"şemsiye)\n",
    "\n",
    "The assumption we are making is: **The meaning of documents are a function of the words they contain**  \n",
    "Let's build that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7ee1ee-9f03-4ce7-89e3-675df596af8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample texts\n",
    "texts = df[\"Message\"].values\n",
    "\n",
    "# Create term-document matrix\n",
    "vectorizer = CountVectorizer()\n",
    "matrix = vectorizer.fit_transform(texts)\n",
    "\n",
    "# Convert to DataFrame for better visualization\n",
    "td_matrix = pd.DataFrame(matrix.toarray(), \n",
    "                 columns=vectorizer.get_feature_names_out(),\n",
    "                 index=[f'Doc{i+1}' for i in range(len(texts))])\n",
    "\n",
    "print(\"Term-document matrix:\")\n",
    "td_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ebfc00-ba5b-40c6-a29d-ab1d7d3fd403",
   "metadata": {},
   "source": [
    "Very simple. You see the rows are documents and the columns are 'words'. We have word representations based on which documents they occur in (is this language modelling??) and we have document representations based on how many of each word they contain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fdb7f7-cefb-46fc-8ac1-f89e3a1933e2",
   "metadata": {},
   "source": [
    "let's see the most frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b59e2c-502f-4e3d-a865-6add0964c854",
   "metadata": {},
   "outputs": [],
   "source": [
    "td_matrix.sum().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3768aac0-77a5-466d-b5ff-8b03eeede77a",
   "metadata": {},
   "source": [
    "We can infer our mostly co-occured words via vector similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f849c36-efe2-471e-b1cf-dd7c25a1f08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_cooccurances(word, matrix_df, top_k=10):\n",
    "    if word not in matrix_df.columns:\n",
    "        raise Exception(f\"{word} does not exist in the vocabulary\")\n",
    "    vec = td_matrix[word].values\n",
    "    similarities = vec.dot(matrix.toarray())\n",
    "    top_k_indices = (-similarities).argsort()[:top_k]\n",
    "    return [matrix_df.iloc[:, i].name for i in top_k_indices]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bcab1a50-257e-4ff5-9677-72160c29fce2",
   "metadata": {},
   "source": [
    "go ahead and discover what words co occur mostly, you can filter by spamness to infer how cooccurance differs between two classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8cd4b7-88ce-4832-8ccd-d3ee252c37e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_most_cooccurances(\"cumalar\", td_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6d0f06-3314-4338-b8c7-4003aa8f6d59",
   "metadata": {},
   "source": [
    "It sort of makes sense, but why is it so ugly?  \n",
    "The words are extracted naively. There are different vectors for \"düşünmek\", \"düşünüyorum\", \"düşündüler\" etc.\n",
    "Our assumption was that document meaning is a function of its words. We can also assume these words would contribute to similar meanings in a document, so treating them as seperate creates noise.  \n",
    "Also, words like \"ve\", \"veya\", \"şöyle\", \"böyle\" should contribute very little to the meaning. Getting rid of those should also remove some of the noise in the matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2a93605a-a7e5-4a2e-8a32-373f42996e07",
   "metadata": {},
   "source": [
    "### Exercise! go ahead and write preprocessing step to mitigate the problems stated above, you might remember terms like stop words, stemming and so on. Then, use your new and beautiful term occurances as features for the classifiers above and see how well it performs compared to your initial feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab34f2e-7583-4d7a-8100-67ffedb68c0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff45bca1-bb92-4821-a1d6-1b856c7a73e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4808fef8-3d1f-4fdd-b941-fb6315b5ffcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219e0423-6470-414b-add2-93cdc62676f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a3a7c6-5880-47ca-a6a9-68bb4fc9519d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ef7dd7-d3c3-4909-b76d-7d15199aefc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e56b391-cc76-4851-9da6-28e334388372",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b732bb0-92cf-496d-b847-678881b94697",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f474b5a3-3249-446a-b443-e230cb9811b7",
   "metadata": {},
   "source": [
    "# NLP Tasks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dda9b40b-0e98-4c82-a308-bfa4e1edd871",
   "metadata": {},
   "source": [
    "## Token Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfa73f36-5256-4b65-a30a-1aca79d88bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488392df-9d17-4534-87a4-1236dbf2f91d",
   "metadata": {},
   "source": [
    "a tokenizer will be useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a420a991-2adf-48d8-a6d7-f51d63073346",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ygzipekci/Library/Application Support/Satyrn/venvs/test_098f6bcd/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "model_name = \"dbmdz/distilbert-base-turkish-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a96a870e-ed5d-4764-a89e-2ac032009f12",
   "metadata": {},
   "source": [
    "### NER"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b7d9fe2b-79ec-4399-8a69-ea8bbcda39de",
   "metadata": {},
   "source": [
    "Token classification is about classifying the parts (words, subwords...) of a text.\n",
    "\n",
    "Most known application is Named Entity Recognition:\n",
    "\n",
    "- [ \"My\", \"name\", \"is\", \"Ahmet\", \".\" ]\n",
    "- [ \"O\", \"O\", \"O\", \"PERSON\", \"O\" ]  \n",
    "\n",
    "Named entity recognition finds the special entities in a text, such as \"person\", \"location\", \"date\".\n",
    "\n",
    "It is a type of token classification, classes being, for example, \"O\", \"PERSON\", \"LOC\", \"DATE\"."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5032b0f8-e34c-43b5-a154-dcb7265eff4c",
   "metadata": {},
   "source": [
    "#### How does the ner data look like?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "68794c5b-4786-4793-a94f-2eb18a29f561",
   "metadata": {},
   "source": [
    "[turkish-nlp-suite/turkish-wikiNER](https://huggingface.co/datasets/turkish-nlp-suite/turkish-wikiNER)  \n",
    "[aynısının github linki](https://github.com/turkish-nlp-suite/Turkish-Wiki-NER-Dataset/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05caf30c-bbae-4ef2-8fe7-4434bf46c8ff",
   "metadata": {},
   "source": [
    "I am reading the same data as pandas dataframe and huggingface Datasets to understand what Datasets has to offer and how do they differ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63cd07f7-cb50-45a8-a511-74e3b8a4a69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading dataset via pandas\n",
    "import pandas as pd\n",
    "\n",
    "splits = {'train': 'dataset/train.json', 'validation': 'dataset/valid.json', 'test': 'dataset/test.json'}\n",
    "df = pd.read_json(\"hf://datasets/turkish-nlp-suite/turkish-wikiNER/\" + splits[\"train\"], lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b5b673-e6c5-4f64-b58c-a1016bc8d36d",
   "metadata": {},
   "source": [
    "These are the classes represented in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb51defb-3907-451a-9cdd-3e53f64c6471",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = ['O',\n",
    "'B-CARDINAL',\n",
    "'I-CARDINAL',\n",
    "'B-DATE',\n",
    "'I-DATE',\n",
    "'B-EVENT',\n",
    "'I-EVENT',\n",
    "'B-FAC',\n",
    "'I-FAC',\n",
    "'B-GPE',\n",
    "'I-GPE',\n",
    "'B-LANGUAGE',\n",
    "'I-LANGUAGE',\n",
    "'B-LAW',\n",
    "'I-LAW',\n",
    "'B-LOC',\n",
    "'I-LOC',\n",
    "'B-MONEY',\n",
    "'I-MONEY',\n",
    "'B-NORP',\n",
    "'I-NORP',\n",
    "'B-ORDINAL',\n",
    "'I-ORDINAL',\n",
    "'B-ORG',\n",
    "'I-ORG',\n",
    "'B-PERCENT',\n",
    "'I-PERCENT',\n",
    "'B-PERSON',\n",
    "'I-PERSON',\n",
    "'B-PRODUCT',\n",
    "'I-PRODUCT',\n",
    "'B-QUANTITY',\n",
    "'I-QUANTITY',\n",
    "'B-TIME',\n",
    "'I-TIME',\n",
    "'B-TITLE',\n",
    "'I-TITLE',\n",
    "'B-WORK_OF_ART',\n",
    "'I-WORK_OF_ART']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6af039-63ff-455f-8c08-f26d1bbe676c",
   "metadata": {},
   "source": [
    "Let's take a look at what we are dealing with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e05f9f38-9510-408f-8c09-590599c23a01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Orda, Spike, ,, First'ün, etkisiyle, Buffy'ye...</td>\n",
       "      <td>[O, B-PERSON, O, B-PERSON, O, B-PERSON, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[\", Macera, edebiyatın, ilk, günlerinden, beri...</td>\n",
       "      <td>[O, O, O, B-ORDINAL, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Günümüzde, Adana'da, 514, okul, öncesi, eğiti...</td>\n",
       "      <td>[O, B-GPE, B-CARDINAL, O, O, O, O, B-CARDINAL,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[11, Temmuz, 1927, tarihinde, Filistin'de, mey...</td>\n",
       "      <td>[B-DATE, I-DATE, I-DATE, I-DATE, B-GPE, B-EVEN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Refrain, Lys, Assia, tarafından, söylenen, ve...</td>\n",
       "      <td>[B-PERSON, I-PERSON, I-PERSON, O, O, O, B-EVEN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17962</th>\n",
       "      <td>[Çoğu, ale, nebatî, lezzetin, kaynağı, olan, m...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17963</th>\n",
       "      <td>[Amancio, önce, Real, Madrid'in, alt, takımı, ...</td>\n",
       "      <td>[B-PERSON, O, B-ORG, I-ORG, O, O, O, B-ORG, I-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17964</th>\n",
       "      <td>[Avrupa, Komisyonu, :, Parlamentoya, ve, Konse...</td>\n",
       "      <td>[B-ORG, I-ORG, O, O, O, O, O, O, O, O, O, O, O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17965</th>\n",
       "      <td>[En, eski, cinsinin, adı, Teckel'dir, ., 19., ...</td>\n",
       "      <td>[O, O, O, O, O, O, B-DATE, I-DATE, I-DATE, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17966</th>\n",
       "      <td>[1941'de, Sergeant, York, filminde, ,, Gary, C...</td>\n",
       "      <td>[B-DATE, B-WORK_OF_ART, I-WORK_OF_ART, O, O, B...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17967 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tokens  \\\n",
       "0      [Orda, Spike, ,, First'ün, etkisiyle, Buffy'ye...   \n",
       "1      [\", Macera, edebiyatın, ilk, günlerinden, beri...   \n",
       "2      [Günümüzde, Adana'da, 514, okul, öncesi, eğiti...   \n",
       "3      [11, Temmuz, 1927, tarihinde, Filistin'de, mey...   \n",
       "4      [Refrain, Lys, Assia, tarafından, söylenen, ve...   \n",
       "...                                                  ...   \n",
       "17962  [Çoğu, ale, nebatî, lezzetin, kaynağı, olan, m...   \n",
       "17963  [Amancio, önce, Real, Madrid'in, alt, takımı, ...   \n",
       "17964  [Avrupa, Komisyonu, :, Parlamentoya, ve, Konse...   \n",
       "17965  [En, eski, cinsinin, adı, Teckel'dir, ., 19., ...   \n",
       "17966  [1941'de, Sergeant, York, filminde, ,, Gary, C...   \n",
       "\n",
       "                                                    tags  \n",
       "0      [O, B-PERSON, O, B-PERSON, O, B-PERSON, O, O, ...  \n",
       "1              [O, O, O, B-ORDINAL, O, O, O, O, O, O, O]  \n",
       "2      [O, B-GPE, B-CARDINAL, O, O, O, O, B-CARDINAL,...  \n",
       "3      [B-DATE, I-DATE, I-DATE, I-DATE, B-GPE, B-EVEN...  \n",
       "4      [B-PERSON, I-PERSON, I-PERSON, O, O, O, B-EVEN...  \n",
       "...                                                  ...  \n",
       "17962  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "17963  [B-PERSON, O, B-ORG, I-ORG, O, O, O, B-ORG, I-...  \n",
       "17964  [B-ORG, I-ORG, O, O, O, O, O, O, O, O, O, O, O...  \n",
       "17965  [O, O, O, O, O, O, B-DATE, I-DATE, I-DATE, O, ...  \n",
       "17966  [B-DATE, B-WORK_OF_ART, I-WORK_OF_ART, O, O, B...  \n",
       "\n",
       "[17967 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b714249-f943-4ad6-a1a4-b73b9b4b305f",
   "metadata": {},
   "source": [
    "Here we see the labels are given for each word. But most modern approaches don't use word tokenization. We also will be using a model with subword tokenization. Subword tokenization is very beneficial with morphologically rich languages like Turkish."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cb95e9-7797-4de4-846e-8e6af643168a",
   "metadata": {},
   "source": [
    "In the function below we are aligning the labels with the actual tokens that our model will use.  \n",
    "Feel free to disect it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22743b54-0294-4b90-b944-d074c8324e46",
   "metadata": {},
   "source": [
    "Here we set the labels of all special tokens to -100 (the index that is ignored by PyTorch) and the labels of all other tokens to the label of the word they come from. Another strategy is to set the label only on the first token obtained from a given word, and give a label of -100 to the other subtokens from the same word. For more info check the [original notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/token_classification.ipynb#scrollTo=DIba90p4rvU_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b160f67-d2fe-474d-944d-3e9b000fbe90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How would the code change if we just assume we only want to label all tokens?\n",
    "\n",
    "label_all_tokens=True\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"],\n",
    "                                 truncation=True,\n",
    "                                 is_split_into_words=True)\n",
    "\n",
    "    word_ids = tokenized_inputs.word_ids()\n",
    "    previous_word_idx = None\n",
    "    label_ids = []\n",
    "    for word_idx in word_ids:\n",
    "         # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "        # ignored in the loss function.\n",
    "        if word_idx is None:\n",
    "            label_ids.append(-100)\n",
    "        # We set the label for the first token of each word.\n",
    "        elif word_idx != previous_word_idx:\n",
    "            label_ids.append(label_list.index(examples[\"tags\"][word_idx]))\n",
    "        # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "        # the label_all_tokens flag.\n",
    "        else:\n",
    "            label_ids.append(label_list.index(examples[\"tags\"][word_idx]) if label_all_tokens else -100)\n",
    "        previous_word_idx = word_idx\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = label_ids\n",
    "    #import pdb; pdb.set_trace()\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07dd10d8-b134-4608-8ec6-0cb8a0640c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 17967/17967 [00:05<00:00, 3491.15it/s]\n"
     ]
    }
   ],
   "source": [
    "tmp_df = df.progress_apply(tokenize_and_align_labels, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54c1d1f4-2ee4-4855-9511-9ae25a70e524",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_df = pd.DataFrame(tmp_df.tolist()) # burayı başka bi şekilde yap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febbba24-9fc7-48ef-a1e3-3bfac16b25a3",
   "metadata": {},
   "source": [
    "This is how the tokenized labels look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4bc54849-f955-4ee2-839d-b422623ec12e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[2, 2757, 1986, 6639, 15453, 16, 9379, 2033, 1...</td>\n",
       "      <td>[-100, 0, 0, 27, 27, 0, 27, 27, 27, 27, 0, 27,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[2, 6, 6496, 5004, 25409, 1009, 2411, 14955, 1...</td>\n",
       "      <td>[-100, 0, 0, 0, 0, 0, 21, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[2, 9236, 6280, 11, 2054, 8236, 1119, 3441, 58...</td>\n",
       "      <td>[-100, 0, 9, 9, 9, 1, 1, 0, 0, 0, 0, 0, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[2, 2974, 4214, 8337, 1106, 3863, 7152, 11, 20...</td>\n",
       "      <td>[-100, 3, 4, 4, 4, 4, 9, 9, 9, 5, 6, 6, 6, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[2, 11123, 2079, 1973, 29764, 1022, 3001, 2740...</td>\n",
       "      <td>[-100, 27, 27, 27, 28, 28, 28, 28, 0, 0, 0, 5,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17962</th>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[2, 12077, 2088, 1025, 22454, 2001, 1089, 2620...</td>\n",
       "      <td>[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17963</th>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[2, 16079, 2329, 1033, 2478, 13314, 12926, 11,...</td>\n",
       "      <td>[-100, 27, 27, 27, 0, 23, 24, 24, 24, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17964</th>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[2, 3070, 7653, 30, 26431, 2029, 1992, 18799, ...</td>\n",
       "      <td>[-100, 23, 24, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17965</th>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[2, 2654, 3275, 21815, 6717, 3668, 15806, 6848...</td>\n",
       "      <td>[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 4, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17966</th>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[2, 7253, 1077, 11, 2012, 25401, 23804, 1009, ...</td>\n",
       "      <td>[-100, 3, 3, 3, 3, 37, 37, 37, 37, 38, 0, 0, 2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17967 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          attention_mask  \\\n",
       "0      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "3      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "4      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "...                                                  ...   \n",
       "17962  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "17963  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "17964  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "17965  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "17966  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                               input_ids  \\\n",
       "0      [2, 2757, 1986, 6639, 15453, 16, 9379, 2033, 1...   \n",
       "1      [2, 6, 6496, 5004, 25409, 1009, 2411, 14955, 1...   \n",
       "2      [2, 9236, 6280, 11, 2054, 8236, 1119, 3441, 58...   \n",
       "3      [2, 2974, 4214, 8337, 1106, 3863, 7152, 11, 20...   \n",
       "4      [2, 11123, 2079, 1973, 29764, 1022, 3001, 2740...   \n",
       "...                                                  ...   \n",
       "17962  [2, 12077, 2088, 1025, 22454, 2001, 1089, 2620...   \n",
       "17963  [2, 16079, 2329, 1033, 2478, 13314, 12926, 11,...   \n",
       "17964  [2, 3070, 7653, 30, 26431, 2029, 1992, 18799, ...   \n",
       "17965  [2, 2654, 3275, 21815, 6717, 3668, 15806, 6848...   \n",
       "17966  [2, 7253, 1077, 11, 2012, 25401, 23804, 1009, ...   \n",
       "\n",
       "                                                  labels  \n",
       "0      [-100, 0, 0, 27, 27, 0, 27, 27, 27, 27, 0, 27,...  \n",
       "1      [-100, 0, 0, 0, 0, 0, 21, 0, 0, 0, 0, 0, 0, 0,...  \n",
       "2      [-100, 0, 9, 9, 9, 1, 1, 0, 0, 0, 0, 0, 1, 1, ...  \n",
       "3      [-100, 3, 4, 4, 4, 4, 9, 9, 9, 5, 6, 6, 6, 0, ...  \n",
       "4      [-100, 27, 27, 27, 28, 28, 28, 28, 0, 0, 0, 5,...  \n",
       "...                                                  ...  \n",
       "17962  [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "17963  [-100, 27, 27, 27, 0, 23, 24, 24, 24, 0, 0, 0,...  \n",
       "17964  [-100, 23, 24, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...  \n",
       "17965  [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 4, ...  \n",
       "17966  [-100, 3, 3, 3, 3, 37, 37, 37, 37, 38, 0, 0, 2...  \n",
       "\n",
       "[17967 rows x 3 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can also add the decoded input_ids to peep into the tokenization of the actual text\n",
    "tokenized_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "151b2ae5-9bb4-466d-bea5-9bfb4c953fc9",
   "metadata": {},
   "source": [
    "#### Finetuning NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca4a2216-385f-40f7-a85c-a90557962a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "23c965f6-0e98-45ee-9748-c91624dd3acc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at dbmdz/distilbert-base-turkish-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=len(label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac679f65-824c-4406-90b7-212cb97c6c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "# Data collator that will dynamically pad the inputs received, as well as the labels.\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fc3604-002a-4df4-838b-bea1ca794579",
   "metadata": {},
   "source": [
    "It is a very convenient abstraction to use datasets library with transformers feel free to check how it differs from pandas df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5c9238d6-c31c-469b-a327-b6a7563d7edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5390\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "split = round(len(tokenized_df)*0.3)\n",
    "print(split)\n",
    "\n",
    "dataset = datasets.DatasetDict(\n",
    "    {\n",
    "        \"train\": datasets.Dataset.from_pandas(tokenized_df[split:]),\n",
    "        \"test\": datasets.Dataset.from_pandas(tokenized_df[:split]),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9749dbf0-467f-44c1-b9ab-d8386deeb6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    \"test-ner\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fab82ac-b4ba-4ee6-87b9-da29d9e81da4",
   "metadata": {},
   "source": [
    "The last thing to define for our Trainer is how to compute the metrics from the predictions. Here we will load the seqeval metric (which is commonly used to evaluate results on the CONLL dataset) via the Datasets library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad08322-c37a-48b3-b909-a2ea9c7e6bf2",
   "metadata": {},
   "source": [
    "So we will need to do a bit of post-processing on our predictions:\n",
    "- select the predicted index (with the maximum logit) for each token\n",
    "- convert it to its string label\n",
    "- ignore everywhere we set a label of -100\n",
    "\n",
    "The following function does all this post-processing on the result of `Trainer.evaluate` (which is a namedtuple containing predictions and labels) before applying the metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "228888d0-5fcf-4382-8499-6bed790e7d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from seqeval.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        \"precision\": precision_score(y_true=true_labels, y_pred=true_predictions),\n",
    "        \"recall\": recall_score(y_true=true_labels, y_pred=true_predictions),\n",
    "        \"f1\": f1_score(y_true=true_labels, y_pred=true_predictions),\n",
    "        \"accuracy\": accuracy_score(y_true=true_labels, y_pred=true_predictions)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d19f8a80-2390-45bf-8841-f7ee8f3b954a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d1/rnh2xqz17_b5l4bh0_zxxpg00000gn/T/ipykernel_7485/2808303626.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8774a7c-5647-4532-b055-92ce3d3288ef",
   "metadata": {},
   "source": [
    "I hope the next cell does not start your pc fan immediately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d0633d4d-5bd9-4e35-a11b-63559f2ac957",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12/12 00:05, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.032189</td>\n",
       "      <td>0.146341</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.707692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.023219</td>\n",
       "      <td>0.170732</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.715385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.009742</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.218750</td>\n",
       "      <td>0.715385</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=12, training_loss=1.222005049387614, metrics={'train_runtime': 6.7247, 'train_samples_per_second': 22.306, 'train_steps_per_second': 1.784, 'total_flos': 1662345126540.0, 'train_loss': 1.222005049387614, 'epoch': 3.0})"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8c85aa-f13d-4911-9a70-9a729e43446d",
   "metadata": {},
   "source": [
    "Let's see how we did on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0162bae2-fc98-4cb6-a15f-874d327ba9c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.009742021560669,\n",
       " 'eval_precision': 0.16666666666666666,\n",
       " 'eval_recall': 0.3181818181818182,\n",
       " 'eval_f1': 0.21874999999999997,\n",
       " 'eval_accuracy': 0.7153846153846154,\n",
       " 'eval_runtime': 0.0469,\n",
       " 'eval_samples_per_second': 106.601,\n",
       " 'eval_steps_per_second': 21.32,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f03ff172-a8a0-40d7-9dfb-33cf7129c07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_test_results():\n",
    "    predictions, labels, _ = trainer.predict(dataset[\"test\"])\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    return true_predictions, true_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "834b36f7-e55f-4696-83b6-d46ba4bacf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred, label = compute_test_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b343a5d-8997-49b2-b07b-c4bd6bb16b2a",
   "metadata": {},
   "source": [
    "We can see example-wise accuracy scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "05c31e52-557c-417b-b469-2c37801066e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.5555555555555556\n",
      "0.8888888888888888\n",
      "0.5909090909090909\n",
      "0.6216216216216216\n"
     ]
    }
   ],
   "source": [
    "for p, l in zip(pred, label):\n",
    "    a = [pp==ll for pp,ll in zip(p,l)]\n",
    "    print(sum(a)/len(a))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3ff3d049-6ff8-49c2-a527-f45d72132cad",
   "metadata": {},
   "source": [
    "#### NER Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08f1f2f-5e2f-4dbb-82be-46066c2381f7",
   "metadata": {},
   "source": [
    "Feel free to play with your examples to see what the model is good and bad at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "42e45767-7cc5-4bca-b549-9b7ce6376222",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_sentence = \"Inzva'nın Taksim binasını Yağız hiç görmemiş.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "4b6a1b91-bdaf-414e-804e-ac4938b56683",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(example_sentence, return_tensors=\"pt\", add_special_tokens=True)\n",
    "inputs[\"input_ids\"] = inputs[\"input_ids\"].to(device=model.device)\n",
    "inputs[\"attention_mask\"] = inputs[\"attention_mask\"].to(device=model.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "712dd0ea-7952-44ab-a17e-dbccd9eb83c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "853ece24-f61f-423d-b3fe-519fba3f7a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_classes = outputs['logits'].argmax(axis=2).cpu().numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "3c6a64eb-de43-4782-bd1e-dcbd7891e989",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(ids=inputs[\"input_ids\"].cpu().numpy()[0], skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c98c7585-07b9-42ba-be28-e370ac18eea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In ----> B-PERSON\n",
      "##z ----> B-PERSON\n",
      "##va ----> B-GPE\n",
      "' ----> B-GPE\n",
      "nın ----> B-GPE\n",
      "Taksim ----> B-GPE\n",
      "binası ----> O\n",
      "##nı ----> O\n",
      "Yağı ----> B-GPE\n",
      "##z ----> O\n",
      "hiç ----> O\n",
      "görmemiş ----> O\n",
      ". ----> O\n"
     ]
    }
   ],
   "source": [
    "for i, p in enumerate(predicted_classes):\n",
    "    if tokens[i] in [tokenizer.sep_token, tokenizer.cls_token]:\n",
    "        continue\n",
    "    print(f\"{tokens[i]} ----> {label_list[p]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "46711946-81ee-4cc1-b710-032d72db174b",
   "metadata": {},
   "source": [
    "### Extractive QA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e30a54a-731e-4a0d-a772-7c55663ecd49",
   "metadata": {},
   "source": [
    "Extractive QA can also be formulated as a token classification problem. Here extractive means that the answers is a span inside the given context. So we can train a model to predict for each token to find which token is the start token and which token is the end token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378fdb7d-d41d-46df-b6ea-47f1bf4f7267",
   "metadata": {},
   "source": [
    "This is what the SQuAD data format looks like which is quite a common standard dataset and format for QA literature (a bit outdated imo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2849a6bc-a8b6-4c4b-878f-69db04c31cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_qa = {\n",
    "                \"data\": [\n",
    "                    {\n",
    "                        \"title\": \"Example\",\n",
    "                        \"paragraphs\": [\n",
    "                            {\n",
    "                                \"context\": \"The quick brown fox jumps over the lazy dog.\",\n",
    "                                \"qas\": [\n",
    "                                    {\n",
    "                                        \"question\": \"What does the fox jump over?\",\n",
    "                                        \"id\": \"q1\",\n",
    "                                        \"answers\": [\n",
    "                                            {\n",
    "                                                \"text\": \"the lazy dog\",\n",
    "                                                \"answer_start\": 32\n",
    "                                            }\n",
    "                                        ]\n",
    "                                    }\n",
    "                                ]\n",
    "                            }\n",
    "                        ]\n",
    "                    }\n",
    "                ],\n",
    "                \"version\": \"2.0\"\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866ea63c-e07c-47a0-ae11-7a1a46b42e2f",
   "metadata": {},
   "source": [
    "We will be demonstrating the Extractive QA Task with a translated SQuAD dataset. From our friends at Boun-tabilab\n",
    "[boun-tabi/squad_tr](https://huggingface.co/datasets/boun-tabi/squad_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d3529dc-8a2b-4f1d-b725-f32a7979fbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "\n",
    "with gzip.open(\"SQuAD-TR/data/squad-tr-train-v1.0.0.json.gz\", \"r\") as f:\n",
    "   qa_data = json.loads(f.read().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8e4dd6-4836-4f40-8c45-e2b11bdfa315",
   "metadata": {},
   "source": [
    "This time we are directly jumping into the HF datasets format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ba2e120-2720-4ee4-b67f-eb31344359b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ygzipekci/Library/Application Support/Satyrn/venvs/test_098f6bcd/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "def json_to_dataset(data):\n",
    "    datalist = []\n",
    "    for title in tqdm(data):\n",
    "        for paragraph in title[\"paragraphs\"]:\n",
    "            for qa in paragraph[\"qas\"]:\n",
    "                if len(qa[\"answers\"]) == 0: # bunları dahil edip de kurgulanabilir aslında\n",
    "                    continue\n",
    "                example = {'id': qa['id'], 'title': title[\"title\"], 'context': paragraph['context'], 'question': qa['question'], 'answers': qa['answers'][0]}\n",
    "                datalist.append(example)\n",
    "    \n",
    "    return Dataset.from_list(datalist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b57b7f3e-9e9d-4dcc-8a54-dee56eaed2d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 754.64it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 10/10 [00:00<00:00, 1167.97it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "squad_tr = json_to_dataset(qa_data[\"data\"][:1]) # I am limiting the number of titles to 10 for faster computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55164fa0-b093-479d-8a9d-e01c44f059bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "    num_rows: 515\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_tr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "91349db0-f059-433a-a798-338db79483b3",
   "metadata": {},
   "source": [
    "Split the dataset's `train` split into a train and test set with the [train_test_split](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.train_test_split) method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32ca1b0e-5a95-4e45-9e3e-7bd9e67baa6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_tr = squad_tr.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ddda31b5-4c41-4083-ad63-432a61356a71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 412\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 103\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1f5b053-4887-407d-9f51-9f07729325a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '56bea5f23aeaaa14008c91a2',\n",
       " 'title': 'Beyonce',\n",
       " 'context': \"Beyoncé'nin ilk olarak Jay Z ile “'03 Bonnie & Clyde” adlı işbirliğinin ardından, yedinci albümü The Blueprint 2: The Gift & The Curse (2002) 'de yer aldığı düşünülmektedir. Beyoncé, şarkıdaki klipte Jay Z'nin kız arkadaşı olarak göründü ve bu da ilişkilerinin spekülasyonlarını daha da artıracak. 4 Nisan 2008'de Beyoncé ve Jay Z tanıtım olmadan evlendi. Nisan 2014 itibariyle çift birlikte 300 milyon rekoru sattı. Çift, son yıllarda daha rahat görünmesine rağmen, özel ilişkileri ile tanınıyor. Beyoncé 2010 veya 2011'de düşük yaptı ve bunu “şimdiye kadar dayandığı en üzücü şey” olarak nitelendirdi. Kaybıyla başa çıkmak için stüdyoya döndü ve müzik yazdı. Nisan 2011'de Beyoncé ve Jay Z, albüm kapağını çekmek için Paris'e gittiler ve beklenmedik bir şekilde Paris'te hamile kaldılar.\",\n",
       " 'question': 'Beyonce nerede hamile kaldı?',\n",
       " 'answers': {'answer_start': 720, 'text': 'Paris'}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_tr[\"train\"][0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "22ef8226-3bd9-4fc8-a76f-b2fae73fa1f6",
   "metadata": {},
   "source": [
    "There are several important fields here:\n",
    "\n",
    "- `answers`: the starting location of the answer token and the answer text.\n",
    "- `context`: background information from which the model needs to extract the answer.\n",
    "- `question`: the question a model should answer.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f634532-2712-4964-9138-cba3f3130993",
   "metadata": {},
   "source": [
    "#### Preprocesing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16e0dc02-0933-4bef-8b0a-ffbdad93caf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/distilbert-base-turkish-cased\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4c21a252-b00f-41e9-8313-639cc6b8f70d",
   "metadata": {},
   "source": [
    "There are a few preprocessing steps particular to question answering tasks you should be aware of:\n",
    "\n",
    "1. Some examples in a dataset may have a very long `context` that exceeds the maximum input length of the model. To deal with longer sequences, truncate only the `context` by setting `truncation=\"only_second\"`.\n",
    "2. Next, map the start and end positions of the answer to the original `context` by setting\n",
    "   `return_offset_mapping=True`.\n",
    "3. With the mapping in hand, now you can find the start and end tokens of the answer. Use the [sequence_ids](https://huggingface.co/docs/tokenizers/main/en/api/encoding#tokenizers.Encoding.sequence_ids) method to\n",
    "   find which part of the offset corresponds to the `question` and which corresponds to the `context`.\n",
    "\n",
    "Here is how you can create a function to truncate and map the start and end tokens of the `answer` to the `context`:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39501ff9-4cd9-4ff5-a502-6dcc200f4952",
   "metadata": {},
   "source": [
    "I recommend checking the videos [here](https://huggingface.co/docs/transformers/tasks/question_answering) for grasping the data format for extractive QA, I based most of this section of notebook from that tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6b75db9-ba9f-4ea0-947c-e31f67e45568",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=384,\n",
    "        truncation=\"only_second\",\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        answer = answers[i]\n",
    "        #start_char = answer[\"answer_start\"][0]\n",
    "        #end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "        start_char = answer[\"answer_start\"]\n",
    "        end_char = answer[\"answer_start\"] + len(answer[\"text\"])\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # If the answer is not fully inside the context, label it (0, 0)\n",
    "        if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0f9711e-7f2b-46e0-9042-d36ffa2220eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|███████████████████████████| 412/412 [00:00<00:00, 2689.44 examples/s]\n",
      "Map: 100%|███████████████████████████| 103/103 [00:00<00:00, 2198.97 examples/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Map:  37%|███▋      | 1000/2696 [00:00<00:00, 2669.54 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Map:  74%|███████▍  | 2000/2696 [00:00<00:00, 2700.55 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Map: 100%|██████████| 2696/2696 [00:00<00:00, 2711.28 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Map: 100%|██████████| 2696/2696 [00:01<00:00, 2675.30 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Map:   0%|          | 0/675 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Map: 100%|██████████| 675/675 [00:00<00:00, 2493.08 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Map: 100%|██████████| 675/675 [00:00<00:00, 2415.62 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#tokenized_squad = squad.map(preprocess_function, batched=True, remove_columns=squad[\"train\"].column_names)\n",
    "tokenized_squad_tr = squad_tr.map(preprocess_function, batched=True, remove_columns=squad_tr[\"train\"].column_names)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8403c539-7c86-4c3f-b088-25e34e4ffde6",
   "metadata": {},
   "source": [
    "Now create a batch of examples using [DefaultDataCollator](https://huggingface.co/docs/transformers/main/en/main_classes/data_collator#transformers.DefaultDataCollator). Unlike other data collators in 🤗 Transformers, the [DefaultDataCollator](https://huggingface.co/docs/transformers/main/en/main_classes/data_collator#transformers.DefaultDataCollator) does not apply any additional preprocessing such as padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed0bb891-5c7c-4774-a023-ed8ad10f3b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DefaultDataCollator\n",
    "\n",
    "data_collator = DefaultDataCollator()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7164eaf9-1cd3-410c-bbd7-a5fae7fea2fb",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ea99af7-79e9-4cf7-b38a-840f043ce6e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at dbmdz/distilbert-base-turkish-cased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"dbmdz/distilbert-base-turkish-cased\", device_map=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc7874a-351a-4c3a-9c1b-f85a9a48d174",
   "metadata": {},
   "source": [
    "As a side note, let's see what huggingface mean by \"model for question answering\" can you spot the difference between when we read the same model as a base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba7c2627-a6b9-48c5-bcda-83c343866948",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForQuestionAnswering(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): DistilBertSdpaAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08b3ec4b-16de-4049-96d6-be7efc9aae40",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = AutoModel.from_pretrained(\"dbmdz/distilbert-base-turkish-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "623beea2-d0b8-4efb-aeb1-fcaaf90154da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertModel(\n",
       "  (embeddings): Embeddings(\n",
       "    (word_embeddings): Embedding(32000, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (layer): ModuleList(\n",
       "      (0-5): 6 x TransformerBlock(\n",
       "        (attention): DistilBertSdpaAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8150feca-bca7-4cb4-9248-2e81f93713ba",
   "metadata": {},
   "source": [
    "At this point, only three steps remain:\n",
    "\n",
    "1. Define your training hyperparameters in [TrainingArguments](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments). The only required parameter is `output_dir` which specifies where to save your model. You'll push this model to the Hub by setting `push_to_hub=True` (you need to be signed in to Hugging Face to upload your model).\n",
    "2. Pass the training arguments to [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) along with the model, dataset, tokenizer, and data collator.\n",
    "3. Call [train()](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer.train) to finetune your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22baecd9-f1a4-4fe0-be3e-d5ec81f07e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ygzipekci/Library/Application Support/Satyrn/venvs/test_098f6bcd/.venv/lib/python3.10/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/var/folders/d1/rnh2xqz17_b5l4bh0_zxxpg00000gn/T/ipykernel_8243/3965627804.py:11: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"test-squad-tr\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_squad_tr[\"train\"],\n",
    "    eval_dataset=tokenized_squad_tr[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18bb6bb-f11b-47eb-a6a5-3c3d16682e3d",
   "metadata": {},
   "source": [
    "Let's write a function to peep into what our data looks like at this stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6fbe2199-1296-450f-bda2-518072501f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_data_viewer(data):\n",
    "    tokens = data[\"input_ids\"]\n",
    "    padding_start = tokens.index(tokenizer.pad_token_id)\n",
    "    tokens = tokens[:padding_start]\n",
    "\n",
    "    #get the answer within\n",
    "    start = data[\"start_positions\"]\n",
    "    end = data[\"end_positions\"]\n",
    "\n",
    "    for idx, token in enumerate(tokens):\n",
    "        if idx == start:\n",
    "            print(\"<<<\", end=\" \")\n",
    "        print(tokenizer.decode(token), end=\" \")\n",
    "        if idx == end:\n",
    "            print(\">>>\", end=\" \")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "74a103ba-12ae-4948-8db0-d5f0a029c013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] Bebek hangi hastanede teslim edildi ? [SEP] 7 Ocak 2012 ' de Bey ##on ##c ##é , ağır güvenlik altında New York ' taki <<< Len ##ox Hill Hastanesi >>> ' nde Blue I ##v ##y Car ##ter ' ı doğur ##du . İki gün sonra Ja ##y Z , çocuklarına adan ##mış bir şarkı olan “ G ##lor ##y ” yi Life ##and ##tim ##es . com ' da yayınladı . Şarkı , Bey ##on ##c ##é ' nin Blue I ##v ##y ' e hamile kalmadan önce uğradığı düşük de dahil olmak üzere çiftin hamilelik mücadele ##lerini ayrıntı ##landırdı . Blue I ##v ##y ' nin çığlık ##ları şarkının sonuna dahil edildi ve resmi olarak “ B . I . C . ” olarak kabul edildi . “ G ##lor ##y ” Hot R & B / Hip - Hop Son ##gs listesine girdiğinde iki günlük olarak Bill ##board listesine giren en genç kişi oldu . [SEP] "
     ]
    }
   ],
   "source": [
    "input_data_viewer(tokenized_squad_tr[\"train\"][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5f93134e-bf71-4bfb-b37b-2dccef5c13d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
       "        num_rows: 412\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
       "        num_rows: 103\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_squad_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5e2c1173-fa3e-4106-9081-b759fe0e40aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='78' max='78' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [78/78 04:41, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>5.732890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>5.756026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>5.746717</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=78, training_loss=5.684173583984375, metrics={'train_runtime': 285.7818, 'train_samples_per_second': 4.325, 'train_steps_per_second': 0.273, 'total_flos': 121115423729664.0, 'train_loss': 5.684173583984375, 'epoch': 3.0})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bdeb8dda-7597-4fbb-b59d-4092d1cc2a97",
   "metadata": {},
   "source": [
    "#### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1308dd8d-fb0d-462b-ae3e-30a2e24fff30",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"SQuAD veriseti ne zaman yayınlandı?\"\n",
    "context = \"The Stanford Question Answering Dataset yani SQuAD veriseti 2016 yılında akademik bir kıyaslama veriseti olarak yayınlandı ancak içerdiği basit örnekler eleştirilere sebep oldu\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4b38f585-dd89-468f-a4c6-b76f2f34d11f",
   "metadata": {},
   "source": [
    "Tokenize the text and return PyTorch tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c2843fef-9257-4cbd-b611-85892fbb924b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inputs = tokenizer(question, context, return_tensors=\"pt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d75e16f2-a358-4948-9064-fcbc8db1557c",
   "metadata": {},
   "source": [
    "Pass your inputs to the model and return the `logits`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4118c481-bc10-4b1e-aa05-aa5f4ab8589a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f14345f-563c-4ca7-ad0f-ce3d9c339fc5",
   "metadata": {},
   "source": [
    "Get the highest probability from the model output for the start and end positions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "43869e88-ebfb-4e3b-bf71-18459ca72c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_start_index = outputs.start_logits.argmax()\n",
    "answer_end_index = outputs.end_logits.argmax()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "44943378-bffa-4c40-ac15-7b833c1b8e36",
   "metadata": {},
   "source": [
    "Decode the predicted tokens to get the answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1e256334-860f-4801-b67e-8743939061f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'##uAD veriseti ne zaman yayınlandı? [SEP] The Stanford Question Answerin'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
    "tokenizer.decode(predict_answer_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afd4414-d499-49ee-ba7e-bf01f68f451f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbcdfda-7550-4b74-82ac-c5572ad7b875",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec56a29-7373-41cd-8725-a5fdabe23f18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1050c0c6-6acc-4ca5-bf92-25e98c150305",
   "metadata": {},
   "source": [
    "## Sequence Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb025310-7380-448e-9521-e9ca2dc4a9ff",
   "metadata": {},
   "source": [
    "The first example of this notebook was about classification. Sentiment Analysis is one of the most popular sequence classification tasks. Do you think we can formulate a question answering problem as a sequence classification task???"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6809ec9f-0e5c-4ba9-a572-8546a49a69de",
   "metadata": {},
   "source": [
    "### Sentiment analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a35151a1-62a1-4e0a-abf5-67c49a5a8f1a",
   "metadata": {},
   "source": [
    "[winvoker/turkish-sentiment-analysis-dataset](https://huggingface.co/datasets/winvoker/turkish-sentiment-analysis-dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3401cfe2-5807-4508-a503-d654c4c75c8b",
   "metadata": {},
   "source": [
    "Checking the sequence classification class of bert models will give us an idea about how this problem that we tried to solve with ancient methods, can be solved with language models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d6af410a-73be-414d-a714-25a16ba6750b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0bbc4cf5-3af4-47eb-a0dc-2099df76569c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "sc_model = BertForSequenceClassification.from_pretrained(\"dbmdz/bert-base-turkish-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "40c93e8a-8e47-4403-8e88-de5a80e9727c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8b5e3a8d-a65d-49cd-b5b3-d677a5cfb24f",
   "metadata": {},
   "source": [
    "## Language Modeling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8723b9c0-6391-4547-94e3-e560e2213447",
   "metadata": {},
   "source": [
    "### Encoder Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "14876fd0-1308-4384-b7d7-d0faf635bd96",
   "metadata": {},
   "source": [
    "Modern encoder models take a natural language input and return a contextualised representation of the input.\n",
    "(still) The most popular and influencial encoder model is BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b709b5b4-7245-408c-b566-25342abd849d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-base-turkish-cased\")\n",
    "model = AutoModel.from_pretrained(\"dbmdz/bert-base-turkish-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8e2e31-097e-4a61-9993-1218c3ada1c8",
   "metadata": {},
   "source": [
    "Let's see what happens to our text input when passed through an encoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "889ee66e-ea1f-4d89-abec-708f87dc24f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 464 ms, sys: 287 ms, total: 752 ms\n",
      "Wall time: 338 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "s = \"Bir zamanlar BERTten büyük dil model diye bahsedilirdi...\"\n",
    "inputs = tokenizer(s, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c8ad39-624a-4418-8dca-ba0995cd80df",
   "metadata": {},
   "source": [
    "inputs are familiar at this point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "95aca1e6-029b-4664-a665-62b221d329e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   2, 2281, 7476,   38, 2864, 1070, 2324, 2368, 3004, 3424, 2636, 7808,\n",
       "         3061, 2016,   18,   18,   18,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64198ab8-8f6d-4dc8-a0bb-c06fefad3e2d",
   "metadata": {},
   "source": [
    "Let's see what the outputs have to offer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "11b1e0a6-44f3-4039-a13e-c499b7740bef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['last_hidden_state', 'pooler_output', 'hidden_states', 'past_key_values', 'attentions', 'cross_attentions'])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.__dict__.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f758f5-ff9d-4a42-ad5c-e26ecdb17d7c",
   "metadata": {},
   "source": [
    "Let's dive into what are those and what use they have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8873d519-6b22-4981-ba05-20b215b58b97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 18, 768])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a179c4ae-a97e-42f9-bb03-1eb2df307155",
   "metadata": {},
   "source": [
    "Last hidden state of BERT is shaped like [batch_size, input_token_size, embedding_size] so it generates an embedding vector for each token, which we have utilized for token classification tasks before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0d206f31-f67c-48f4-99e2-543619c82b62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.pooler_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730239cd-8e89-4713-9c3d-5d315aa7fcf2",
   "metadata": {},
   "source": [
    "Pooler output is (although implementations may differ between bert variants) the CLS token embedding went through a linear layer and tanh activation. This is mostly used for sentence embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "75c791a3-8a7d-4dc5-896b-b6182b27f8c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.3975e-02,  9.9932e-01,  8.3639e-02,  1.0548e-04,  9.9713e-01,\n",
       "          1.0797e-01, -9.9961e-01, -9.9946e-01,  8.3098e-01, -2.0310e-01,\n",
       "         -1.7842e-01, -2.4855e-01,  6.7193e-01,  7.0742e-02,  1.8804e-01,\n",
       "         -5.2339e-02, -9.9978e-01, -1.4373e-02,  9.9947e-01,  9.9994e-01,\n",
       "          2.1633e-01,  2.1890e-03, -9.9988e-01, -2.3523e-01,  2.0793e-01,\n",
       "         -9.9994e-01,  9.8377e-01, -9.9876e-01,  1.0000e+00, -9.9643e-01,\n",
       "         -9.9996e-01, -4.0036e-02,  9.8537e-01, -9.7859e-01,  9.9999e-01,\n",
       "         -9.9997e-01, -9.9908e-01,  3.7955e-02, -1.2385e-01,  9.9600e-01,\n",
       "          5.3146e-01, -9.4441e-03,  9.9473e-01,  9.9999e-01,  1.0000e+00,\n",
       "          9.5748e-01,  6.1106e-02,  1.4361e-01, -9.8359e-01, -4.2956e-01,\n",
       "         -9.8889e-01, -9.2030e-01, -4.9239e-02,  9.9492e-01, -1.1915e-01,\n",
       "         -4.8750e-02,  1.0000e+00, -1.4786e-03,  5.3366e-03,  1.1981e-01,\n",
       "         -6.2311e-02, -9.9991e-01,  9.9936e-01,  1.0583e-01,  5.7853e-02,\n",
       "         -2.5122e-01,  9.9771e-01,  9.8273e-01, -9.9869e-01,  9.9991e-01,\n",
       "         -9.9597e-01, -8.8926e-02,  6.1467e-02,  9.7653e-01,  9.7595e-01,\n",
       "         -5.4382e-02, -9.9993e-01, -6.8169e-02, -2.2930e-01, -8.4113e-01,\n",
       "          9.9876e-01, -5.9612e-02, -4.5760e-02,  1.3508e-01, -9.9988e-01,\n",
       "         -4.4657e-02, -1.0000e+00, -5.7769e-02, -9.9993e-01, -1.0000e+00,\n",
       "          9.9998e-01, -9.7805e-02, -2.2113e-02,  4.8062e-02, -3.9094e-02,\n",
       "         -4.7786e-02,  1.0000e+00, -9.9381e-01,  3.9828e-01, -9.8785e-01,\n",
       "         -4.4949e-02, -9.9997e-01, -4.4172e-02,  8.9621e-02, -9.9941e-01,\n",
       "         -7.2483e-02, -9.9976e-01, -5.9840e-01,  1.0000e+00,  9.8810e-01,\n",
       "         -9.0157e-02, -2.5787e-01,  1.0000e+00,  1.2784e-01, -1.3313e-01,\n",
       "          3.3071e-02,  5.6449e-02,  9.9821e-01,  2.1284e-01, -1.0727e-01,\n",
       "          9.9996e-01, -1.0000e+00,  5.1163e-02,  3.0667e-02, -1.0000e+00,\n",
       "         -2.5189e-02, -9.6402e-02,  9.9982e-01,  9.9998e-01, -9.9874e-01,\n",
       "         -1.0534e-01,  9.9997e-01, -9.9961e-01, -1.0000e+00, -3.6757e-02,\n",
       "          1.0983e-01, -4.6806e-02, -1.0000e+00, -1.3389e-01,  9.9898e-01,\n",
       "         -1.5393e-02,  9.9456e-01, -1.0000e+00, -9.9999e-01,  9.9942e-01,\n",
       "          9.9994e-01,  9.9478e-01, -1.6967e-01, -8.6481e-02, -9.9771e-01,\n",
       "         -9.9696e-01,  1.0000e+00, -2.5691e-01, -8.8995e-01,  9.9340e-01,\n",
       "         -9.6991e-01, -1.9158e-02, -9.9236e-02, -9.9898e-01,  9.9999e-01,\n",
       "         -4.4711e-02,  2.0907e-02, -9.8994e-01,  5.2332e-01,  9.9987e-01,\n",
       "         -9.9996e-01,  7.1610e-02, -1.1597e-02,  9.9377e-01,  9.9274e-01,\n",
       "          9.9319e-01,  1.0000e+00,  9.9989e-01,  2.2820e-01, -9.5304e-01,\n",
       "         -8.0133e-02,  7.7242e-02,  1.0000e+00, -9.8509e-01, -9.9999e-01,\n",
       "          8.8001e-01,  4.2934e-02,  1.4902e-01, -1.0000e+00,  9.9969e-01,\n",
       "          9.9998e-01, -1.2043e-01, -5.9946e-01,  8.0999e-03, -2.0161e-01,\n",
       "          7.7810e-02,  8.1180e-01,  9.9994e-01, -1.0245e-01, -9.9959e-01,\n",
       "         -6.6781e-03,  1.9014e-01,  1.4067e-01,  9.9928e-01,  2.1768e-01,\n",
       "          9.1306e-01, -9.9999e-01, -1.0000e+00, -6.0076e-01,  1.0000e+00,\n",
       "          1.6669e-01,  9.8309e-01, -9.9996e-01, -9.7748e-01,  9.9985e-01,\n",
       "         -4.4280e-01,  9.7020e-01, -2.9421e-02, -5.4729e-01, -9.7360e-02,\n",
       "          9.7504e-01, -1.4208e-01, -7.8021e-02,  6.9648e-01,  9.9180e-01,\n",
       "         -9.4319e-01,  1.1182e-03, -8.4737e-02,  9.9971e-01, -9.9913e-01,\n",
       "         -1.2608e-01,  1.3559e-01, -1.1947e-02, -9.8529e-01, -1.6151e-01,\n",
       "          9.9984e-01,  9.9997e-01,  9.9929e-01,  9.2191e-02,  6.9140e-01,\n",
       "          9.8938e-01,  9.9356e-01, -5.5219e-02,  1.0000e+00,  6.7646e-02,\n",
       "         -9.9981e-01, -9.9999e-01,  1.2666e-01, -9.9158e-01, -1.2433e-01,\n",
       "         -1.4530e-02,  5.7801e-01, -3.4735e-02,  1.1741e-01,  1.8666e-02,\n",
       "          1.1751e-01,  9.9997e-01, -9.9866e-01,  9.9223e-01,  9.1732e-01,\n",
       "         -3.7529e-02,  9.9898e-01,  9.9967e-01, -9.9997e-01,  3.6313e-02,\n",
       "          9.9997e-01,  9.9688e-01,  1.2289e-01,  9.5614e-03, -9.9955e-01,\n",
       "         -9.0831e-01, -9.9983e-01,  9.9984e-01,  9.9998e-01,  2.7709e-02,\n",
       "          9.9968e-01, -9.4970e-01,  5.9531e-02, -9.9998e-01, -1.0000e+00,\n",
       "         -9.9990e-01,  7.2351e-01,  2.0033e-01,  1.0000e+00,  9.9777e-01,\n",
       "          1.2483e-01,  1.4399e-02, -1.1834e-01, -9.8827e-01,  8.7473e-01,\n",
       "         -9.9923e-01,  4.0510e-02, -9.9623e-01, -9.9987e-01, -1.9299e-01,\n",
       "         -9.9611e-01, -9.9814e-01,  9.9852e-01,  3.6378e-01,  2.2384e-01,\n",
       "          1.4928e-01, -3.1107e-02,  8.1829e-02,  1.8601e-02,  2.4272e-02,\n",
       "         -6.6192e-02,  8.7718e-01, -8.8687e-01,  9.9503e-01, -9.9999e-01,\n",
       "         -3.7518e-03,  1.5796e-01,  9.9928e-01,  7.0998e-02,  9.9960e-01,\n",
       "          1.0000e+00, -3.1807e-02,  5.3273e-01,  1.1028e-02,  9.9966e-01,\n",
       "          2.1950e-01,  9.6392e-01,  9.3219e-01, -9.9789e-01, -9.9992e-01,\n",
       "          1.0000e+00, -7.3445e-02, -2.9503e-02,  1.0000e+00, -9.9999e-01,\n",
       "          9.6834e-01, -1.2024e-01, -9.9996e-01,  1.4033e-01,  9.9999e-01,\n",
       "         -9.9997e-01, -1.0000e+00, -9.2071e-01,  9.9924e-01,  1.0000e+00,\n",
       "          6.8571e-02,  9.6724e-01,  2.6571e-02,  9.9922e-01,  9.9998e-01,\n",
       "          3.4435e-02,  1.5048e-03, -6.0084e-02,  9.9989e-01,  9.9999e-01,\n",
       "          8.1308e-02, -5.8597e-02, -2.1436e-01,  1.5721e-01,  1.7504e-02,\n",
       "         -1.1561e-01, -8.4927e-01, -9.9999e-01,  9.9999e-01,  9.9999e-01,\n",
       "         -9.9907e-01, -9.9995e-01, -7.3691e-02,  1.1779e-01, -9.0609e-02,\n",
       "          9.9156e-01, -8.7694e-02, -9.9559e-01, -9.9887e-01, -1.8668e-02,\n",
       "         -9.9911e-01,  3.2419e-02,  1.0000e+00,  1.3697e-01, -4.7956e-02,\n",
       "          1.6313e-01,  1.1077e-01,  2.7802e-02,  1.0000e+00,  9.9908e-01,\n",
       "          9.9926e-01, -9.0597e-01, -6.1807e-02, -7.4694e-01,  1.0000e+00,\n",
       "         -9.9857e-01, -2.4920e-01,  1.0000e+00, -6.5812e-01,  9.9996e-01,\n",
       "         -9.9968e-01, -9.9955e-01, -9.9851e-01,  1.1704e-01,  9.9893e-01,\n",
       "          1.4802e-01, -9.9102e-01,  6.4372e-01, -8.1024e-02,  1.6248e-01,\n",
       "          9.9591e-01,  1.0000e+00,  5.2665e-03,  8.7722e-01,  1.5807e-02,\n",
       "          9.9998e-01, -9.3889e-01, -3.9394e-02, -9.9999e-01, -9.9997e-01,\n",
       "         -4.4017e-02,  1.0000e+00,  9.8370e-01, -9.9999e-01, -9.9877e-01,\n",
       "         -1.3342e-01,  9.9967e-01,  9.9830e-01, -9.9732e-01, -9.8865e-02,\n",
       "         -1.3545e-01,  7.3915e-02,  9.9850e-01, -9.7816e-01, -1.0000e+00,\n",
       "          1.2735e-01, -8.4956e-02, -2.2993e-02, -1.6757e-02, -3.4339e-02,\n",
       "         -1.0000e+00, -8.3607e-02, -8.4140e-01, -7.4126e-02,  9.6567e-01,\n",
       "         -6.5827e-02,  9.9892e-01, -9.9985e-01,  9.9771e-01,  1.0122e-01,\n",
       "         -1.7899e-01,  6.6506e-01, -9.2107e-01, -6.6236e-02,  1.1282e-01,\n",
       "          9.9977e-01, -5.4260e-02,  1.0270e-01,  4.7220e-02,  5.4102e-02,\n",
       "         -1.0000e+00, -1.3253e-01, -9.8474e-01, -1.9090e-02, -7.4796e-01,\n",
       "          8.1533e-01, -9.9997e-01,  9.9486e-01,  1.0957e-01, -3.5900e-02,\n",
       "          9.9144e-01, -1.7783e-01, -2.0639e-02, -3.6424e-02, -1.0000e+00,\n",
       "          9.9999e-01, -7.9432e-02,  6.8545e-02,  2.1052e-03, -9.9880e-01,\n",
       "          9.9999e-01, -5.7808e-02, -1.2918e-01,  1.0000e+00, -1.8847e-02,\n",
       "          9.9904e-01, -1.3769e-02, -2.5950e-02, -9.9963e-01,  9.9871e-01,\n",
       "         -6.9692e-03,  9.8942e-01, -1.8400e-01, -2.7700e-02,  2.5779e-01,\n",
       "          9.9999e-01, -1.3956e-01, -9.9972e-01,  4.6142e-02, -1.3595e-01,\n",
       "          9.9830e-01, -9.9799e-01, -1.0000e+00,  1.9347e-01,  1.9976e-02,\n",
       "         -2.8826e-02,  9.9985e-01,  1.2494e-01,  5.7039e-02, -9.9997e-01,\n",
       "         -9.2843e-01, -9.9769e-01, -6.3412e-02, -2.0008e-01,  2.8772e-03,\n",
       "          9.0548e-02,  1.2793e-01, -7.0288e-02,  4.3323e-02, -9.9663e-01,\n",
       "          9.9986e-01,  6.9909e-01, -3.0951e-03,  1.9329e-01,  9.9790e-01,\n",
       "          9.9995e-01, -4.9894e-01, -2.3121e-01, -1.0000e+00, -2.0641e-01,\n",
       "         -9.2929e-02,  1.1486e-01,  1.7997e-02, -2.4356e-02,  5.8604e-02,\n",
       "          1.3817e-01, -2.4735e-01, -9.9996e-01,  9.8851e-01, -3.3354e-01,\n",
       "         -9.9838e-01,  9.9998e-01,  9.9268e-01,  7.9945e-02,  1.3186e-02,\n",
       "          1.3037e-01, -3.6083e-02, -9.9971e-01,  9.9456e-01, -7.2945e-02,\n",
       "          9.7860e-01, -9.8142e-01, -1.0000e+00,  9.9636e-01, -4.0247e-03,\n",
       "          9.9586e-01, -2.3268e-03,  9.7789e-01, -9.0313e-02,  1.0850e-01,\n",
       "          9.9358e-01,  9.6793e-02,  8.2400e-01, -4.4533e-02, -9.5450e-01,\n",
       "         -1.0000e+00, -9.9644e-01, -8.3352e-01,  5.0979e-01,  1.8750e-01,\n",
       "         -9.9998e-01,  9.7742e-01,  8.8825e-02,  9.9896e-01, -9.9835e-01,\n",
       "         -9.9806e-01,  9.0078e-02,  9.9912e-01,  9.9819e-01, -4.8664e-01,\n",
       "         -9.9959e-01, -4.6675e-04,  9.9999e-01, -9.9933e-01, -9.9944e-01,\n",
       "         -1.5828e-04, -5.9475e-02, -1.9112e-01,  9.9997e-01,  1.1045e-01,\n",
       "          6.5558e-02, -7.0722e-02, -9.9979e-01, -1.9860e-01,  9.8653e-01,\n",
       "         -2.0385e-01,  9.9986e-01,  9.8784e-01,  6.5695e-01,  6.6038e-03,\n",
       "          1.2067e-02, -9.9983e-01, -9.9996e-01, -7.9053e-02, -9.9986e-01,\n",
       "         -5.6651e-02,  1.0683e-01,  3.9035e-02, -9.9999e-01,  6.6238e-02,\n",
       "         -1.0000e+00, -1.1149e-01,  8.4934e-02, -9.9966e-01,  9.9997e-01,\n",
       "          2.6344e-01, -2.1122e-02,  8.9849e-02,  9.9875e-01,  6.9108e-02,\n",
       "          4.7826e-02, -9.9998e-01,  2.0127e-02,  3.6707e-01,  9.9849e-01,\n",
       "         -9.9997e-01,  7.7576e-02,  4.8605e-02, -1.0009e-01,  9.2653e-02,\n",
       "         -9.9990e-01,  7.0433e-02,  4.9051e-03,  7.0902e-03, -5.3508e-02,\n",
       "         -9.9939e-01,  9.9831e-01,  4.5454e-02, -3.6742e-02,  4.6031e-02,\n",
       "          1.1931e-01,  9.9410e-01,  9.9919e-01, -1.4833e-01, -6.8217e-02,\n",
       "         -1.4406e-01, -9.7633e-01, -8.0193e-03, -9.9837e-01, -9.8922e-01,\n",
       "         -1.0000e+00, -1.1849e-01, -1.3876e-01,  2.1990e-01,  8.1526e-01,\n",
       "          9.7676e-02,  1.1845e-01,  1.0000e+00,  1.0000e+00,  1.2865e-01,\n",
       "          9.9978e-01,  9.7636e-01,  9.9921e-01, -9.9610e-01, -4.2843e-02,\n",
       "          5.4557e-03, -1.1228e-02, -1.7466e-02, -9.9982e-01, -9.4194e-01,\n",
       "          9.9945e-01,  4.9204e-03, -9.9877e-01, -9.9993e-01, -6.3510e-02,\n",
       "          1.3929e-01, -9.9998e-01, -5.7099e-01, -9.9791e-01,  9.9997e-01,\n",
       "          6.6539e-02, -9.9901e-01,  9.9995e-01, -9.1353e-02,  9.9978e-01,\n",
       "          9.9917e-01, -7.7466e-02,  9.9997e-01, -9.9999e-01,  1.0000e+00,\n",
       "         -6.1860e-02,  9.3418e-01, -3.0148e-02, -8.9929e-01, -7.7506e-02,\n",
       "         -4.6031e-01, -4.9745e-02, -1.0000e+00, -1.3167e-01, -9.9996e-01,\n",
       "          9.9680e-01,  8.6440e-03, -9.1207e-01, -1.5925e-02,  9.1303e-02,\n",
       "         -9.9968e-01, -1.6697e-01,  9.9961e-01,  6.7220e-02,  9.9801e-01,\n",
       "         -2.7756e-01, -1.0000e+00,  9.9828e-01, -1.0000e+00,  1.2704e-01,\n",
       "          9.9381e-01,  1.0146e-01, -1.0000e+00, -2.5981e-03,  9.5136e-01,\n",
       "         -9.6539e-01,  8.5589e-01, -3.6354e-02, -2.4031e-01,  3.3489e-02,\n",
       "         -6.9964e-02, -2.2653e-01,  5.6691e-02,  9.9966e-01,  9.9994e-01,\n",
       "         -9.9981e-01, -5.2334e-02, -9.7834e-01, -9.3939e-01, -8.1210e-02,\n",
       "         -4.2601e-02,  9.9998e-01, -9.9091e-01,  1.5138e-02, -2.7758e-03,\n",
       "          7.2796e-01,  1.0000e+00,  1.0000e+00, -9.9450e-01, -4.6309e-02,\n",
       "         -9.9421e-01, -9.9876e-01, -1.8886e-02, -1.0199e-03, -9.9871e-01,\n",
       "         -1.1152e-01, -9.9968e-01,  1.0000e+00, -1.2626e-02, -6.1708e-02,\n",
       "         -9.9858e-01, -4.3442e-01, -3.6143e-01, -9.9997e-01,  9.9978e-01,\n",
       "         -9.9993e-01,  7.2268e-02,  3.3619e-01, -1.0754e-01, -8.8108e-02,\n",
       "         -1.0000e+00,  6.5305e-02,  4.0433e-02, -8.8699e-01,  2.8663e-01,\n",
       "          2.2003e-01,  3.9120e-02, -3.2212e-02,  1.1375e-01,  3.1074e-02,\n",
       "         -6.3201e-02,  9.9999e-01, -1.0000e+00, -2.8373e-02,  8.0151e-02,\n",
       "         -9.9487e-01, -1.4543e-01,  1.0000e+00]], grad_fn=<TanhBackward0>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.pooler_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df8b0f7-b562-4787-80ba-81a97a7b2aa6",
   "metadata": {},
   "source": [
    "**This is basically a 768 dimentional feature vector. You can use this for the very first problem in this notebook and see how it compares!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95331079-5350-4990-a045-c98788d0c060",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d330850-6363-4045-a4ab-5f668da1c875",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c45d10a7-6b77-4151-b6ad-72b15ba01414",
   "metadata": {},
   "source": [
    "### Encoder - Decoder Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38fb4cf-4346-4481-b87d-49c4a5a3fb38",
   "metadata": {},
   "source": [
    "Encoder - Decoder Models are mostly used for sequence-to-sequence NLP problems. Such as translation, summarization, generative question answering and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a288118-aa22-4714-be2f-eb39d04e8488",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config of the encoder: <class 'transformers.models.bert.modeling_bert.BertModel'> is overwritten by shared encoder config: BertConfig {\n",
      "  \"_name_or_path\": \"dbmdz/bert-base-turkish-cased\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'transformers.models.bert.modeling_bert.BertLMHeadModel'> is overwritten by shared decoder config: BertConfig {\n",
      "  \"_name_or_path\": \"dbmdz/bert-base-turkish-cased\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": true,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-base-turkish-cased\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"ahmetbagci/bert2bert-turkish-paraphrase-generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a05d51c-7c3d-41ef-8aac-6493b72dd1cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderDecoderModel(\n",
       "  (encoder): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (decoder): BertLMHeadModel(\n",
       "    (bert): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(32000, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSdpaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (crossattention): BertAttention(\n",
       "              (self): BertSdpaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (cls): BertOnlyMLMHead(\n",
       "      (predictions): BertLMPredictionHead(\n",
       "        (transform): BertPredictionHeadTransform(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (transform_act_fn): GELUActivation()\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (decoder): Linear(in_features=768, out_features=32000, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38b72466-f822-4462-931c-101db95523b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beni benden alırsa seni bırakmam mümkün mü?\n"
     ]
    }
   ],
   "source": [
    "text=\"beni benden alırsan seni sana bırakmam\"\n",
    "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n",
    "output_ids = model.generate(input_ids)\n",
    "print(tokenizer.decode(output_ids[0], skip_special_tokens=True))\n",
    "#sample output\n",
    "#son model arabalar çevre için daha az zararlı mı?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e3be3904-1860-4f51-9b4f-5d6be84556d6",
   "metadata": {},
   "source": [
    "### Decoder Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ca4219-39a8-4599-b04f-51b78d0ff947",
   "metadata": {},
   "source": [
    "Decoder Models are all the fuzz since chatgpt. Let's look into their workings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49674eac-cfa5-45b1-8ef8-a26c305d25ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee577bc5-5480-4913-8c67-89b0aa27d3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilgpt2\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"distilbert/distilgpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aae5557-7939-4ba4-a35b-f4435231aaec",
   "metadata": {},
   "source": [
    "We are going to look into instruction tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f69d0b7e-5b52-42f7-8e9a-3fcbe1ef8d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"BrewInteractive/alpaca-tr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3934146a-f05e-461a-b9e7-1798aebe2b59",
   "metadata": {},
   "source": [
    "We know that decoder only models are autoregressive next-token predictors. Their task is also called \"document completion\" because the continue writing whatever the input document was.  \n",
    "But how come models that just make more of the input receive dialog capabilities?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e21e5e0e-9cd7-43a9-8074-53fe311eed60",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.chat_template = \"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\"\n",
    "def form_prompts(examples):\n",
    "    prompts = {}\n",
    "    if examples[\"input\"]:\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": examples[\"instruction\"]},\n",
    "            {\"role\": \"context\", \"content\": examples[\"input\"]},\n",
    "            {\"role\": \"assistant\", \"content\": examples[\"output\"]}\n",
    "        ]\n",
    "    else:\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": examples[\"instruction\"]},\n",
    "            {\"role\": \"assistant\", \"content\": examples[\"output\"]}\n",
    "        ]\n",
    "    prompts[\"prompt\"] = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "    prompts[\"input_ids\"] = tokenizer.apply_chat_template(messages, tokenize=True, truncation=True)\n",
    "    return {\"input_ids\": prompts[\"input_ids\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f7b921f2-72fb-40f5-98ec-4569d553d4f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|███████████████████████| 45331/45331 [00:35<00:00, 1269.92 examples/s]\n"
     ]
    }
   ],
   "source": [
    "ds = ds.map(batched_form_prompts, remove_columns=ds[\"train\"].column_names, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b5ee12-2504-4c86-8d39-8d5a51ff9e99",
   "metadata": {},
   "source": [
    "So yes it is still document completion but the document looks in a very specific format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7d93b4e4-7f3e-4497-bbe3-dc8248e2e5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds[\"train\"].train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1368ff21-8a16-40ea-9eb8-140a81e73956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['prompts', 'input_ids'],\n",
       "        num_rows: 36264\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['prompts', 'input_ids'],\n",
       "        num_rows: 9067\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1d7168f3-4453-445b-8433-198b64b27dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "Hello, how are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "I'm doing great. How can I help you today?<|im_end|>\n",
      "<|im_start|>user\n",
      "I'd like to show off how chat templating works!<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chat = [\n",
    "  {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
    "  {\"role\": \"assistant\", \"content\": \"I'm doing great. How can I help you today?\"},\n",
    "  {\"role\": \"user\", \"content\": \"I'd like to show off how chat templating works!\"},\n",
    "]\n",
    "\n",
    "print(tokenizer.apply_chat_template(chat, tokenize=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53134a0d-3418-4001-be8c-9f0cbc52ee8a",
   "metadata": {},
   "source": [
    "Every dialog with any instruction model is parsed into a single string at the background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad11dbfe-e3fb-438d-96c3-2525fed29050",
   "metadata": {},
   "source": [
    "**Extras** What is lora how does it work why does it work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e1faade8-99b4-408a-83d3-514b09ef8079",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-5): 6 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbfd58d-4702-4d37-a848-b02d5e40ce63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "target_modules = [\"c_attn\"]\n",
    "config = LoraConfig(\n",
    "    r=1,\n",
    "    lora_alpha=16, \n",
    "    target_modules=target_modules, \n",
    "    lora_dropout=0.1, \n",
    "    bias=\"none\", \n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "lora_model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521201c8-7651-48ad-aa8e-6963a0a7fb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2a2c07-ec7b-42ba-aded-1d9e83fdcaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling, TrainingArguments, Trainer\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"gpt2_alpaca_tr\",\n",
    "    eval_strategy=\"no\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    use_cpu=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=ds[\"train\"], # datanın neye benzemesi gerekiyo bi bak\n",
    "    eval_dataset=ds[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e26158d-7e99-4fe7-8c88-e09ad95f66b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110d8214-dde2-41a6-ac2b-b3f37a013143",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5d9f6b-5729-4af6-9604-1ceffd2eb088",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "da9a8d6f-17cc-4e4e-863f-e3728081f155",
   "metadata": {},
   "source": [
    "### Inferance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "546e87d9-5ae4-4872-930c-40c5592c3853",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Somatic hypermutation allows the immune system to\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "outputs = model.generate(inputs, max_new_tokens=100, do_sample=True, top_k=50, top_p=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "240f4ef7-4566-4094-a542-340b61678883",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Somatic hypermutation allows the immune system to take its place and treat symptoms caused by mutations, but this treatment fails to take into account the role of a genetic mutation, a phenomenon which often leads to severe adverse effects that can have a detrimental impact on the immune system.\\n\\n\\n\\n\\nThe current study focused on the effect of a combination of genetic modification and an immunomodulatory system on mice with a normal immune system, with an in vitro test to determine the effects on human immune system function in mice of multiple strains and a mouse'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inzva_w1",
   "language": "python",
   "name": "test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
